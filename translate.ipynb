{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path = \"epir_train/\"\n",
    "# index = \"Unnamed: 0\"\n",
    "articles = pd.read_csv(path + 'articles.csv', index_col=\"Unnamed: 0\")\n",
    "\n",
    "# life_situations = pd.read_csv(path + 'life_situations.csv', index_col=\"Unnamed: 0\")\n",
    "# news = pd.read_csv(path + 'news.csv', index_col=\"Unnamed: 0\")\n",
    "# services = pd.read_csv(path + 'services.csv', index_col=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that will be deleted:\n",
      "Empty DataFrame\n",
      "Columns: [id, sys_lang, projects, url, text]\n",
      "Index: []\n",
      "          id sys_lang                           projects  \\\n",
      "20000  71240       kk                  kyzylorda-tabigat   \n",
      "20001    940       kk                                bko   \n",
      "20002  75517       ru      kostanai-karasy-audany-akimat   \n",
      "20003  83645       kk                  kyzylorda-tabigat   \n",
      "20004  61774       en  kostanai-altynsarin-audany-akimat   \n",
      "20005  25698       ru                      aqmola-zhaksy   \n",
      "\n",
      "                                                     url  \\\n",
      "20000  https://www.gov.kz/memleket/entities/kyzylorda...   \n",
      "20001  https://www.gov.kz/memleket/entities/bko/press...   \n",
      "20002  https://www.gov.kz/memleket/entities/kostanai-...   \n",
      "20003  https://www.gov.kz/memleket/entities/kyzylorda...   \n",
      "20004  https://www.gov.kz/memleket/entities/kostanai-...   \n",
      "20005  https://www.gov.kz/memleket/entities/aqmola-zh...   \n",
      "\n",
      "                                                    text  \n",
      "20000  ҚАМ\" БК\" ЖШС мұнай кәсіпшілігі нысандарын кеңе...  \n",
      "20001  Инвестициялар ИНВЕСТИЦИЯЛАР ТАРТУ 2022 жылдың ...  \n",
      "20002  Особенности прекращения деятельности налогопла...  \n",
      "20003  «Шекті Нормативті Рауалы шығарынды (ШРШ) ласта...  \n",
      "20004  Now the tax for a house and a land plot must b...  \n",
      "20005  24 октября Мониторинговой группой  проведен мо...  \n"
     ]
    }
   ],
   "source": [
    "articles = articles.truncate(before=20000, after=20005)\n",
    "# articles\n",
    "def clean_df(df):\n",
    "    text_cols = []\n",
    "    for col in df.columns:\n",
    "        if col not in ['id', 'sys_lang', 'subid', 'URL']:\n",
    "            text_cols.append(col)\n",
    "\n",
    "    df[text_cols] = df[text_cols].astype(str)\n",
    "    mask = df[text_cols].apply(lambda x: x.str.split().str.len() < 10).all(axis=1)\n",
    "\n",
    "    print(\"Rows that will be deleted:\")\n",
    "    print(df[mask])\n",
    "\n",
    "    df = df.drop(df[mask].index)\n",
    "    return df\n",
    "articles = clean_df(articles)\n",
    "def concat_text_columns(df, columns):\n",
    "    df = df.copy()\n",
    "    df[columns] = df[columns].replace('nan', '')\n",
    "    df['text'] = df[columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "    df = df.drop(columns=columns)\n",
    "    return df\n",
    "# articles = concat_text_columns(articles, ['title', 'content'])\n",
    "print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_csv('data/articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_into_chunks(text, max_chunk_size=4500):\n",
    "    # Use regular expressions to split the text into sentences\n",
    "    # Assumes sentences end with \".\", \"!\", or \"?\", followed by a space or newline\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    " \n",
    "    # Initialize variables\n",
    "    current_chunk = \"\"\n",
    "    chunks = []\n",
    " \n",
    "    # Iterate through sentences and add them to the current chunk\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            # If adding the sentence would exceed the chunk size, start a new chunk\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    " \n",
    "    # Add the last chunk, if any\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    " \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/*.csv\n",
      "data/articles.csv\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "Request exception can happen due to an api connection error. Please check your connection and try again",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/yera/projects/pyth/translate/datasaur/translate.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yera/projects/pyth/translate/datasaur/translate.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             df\u001b[39m.\u001b[39mat[ind, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([GoogleTranslator(source\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkk\u001b[39m\u001b[39m'\u001b[39m, target\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtranslate(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m chunks])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yera/projects/pyth/translate/datasaur/translate.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             \u001b[39m# df.at[ind, \"text\"] = \"p\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yera/projects/pyth/translate/datasaur/translate.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m translate(\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/yera/projects/pyth/translate/datasaur/translate.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yera/projects/pyth/translate/datasaur/translate.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m ind, _ \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yera/projects/pyth/translate/datasaur/translate.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     chunks \u001b[39m=\u001b[39m split_string_into_chunks(df\u001b[39m.\u001b[39mat[ind, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yera/projects/pyth/translate/datasaur/translate.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     df\u001b[39m.\u001b[39mat[ind, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([GoogleTranslator(source\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkk\u001b[39m\u001b[39m'\u001b[39m, target\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtranslate(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m chunks])\n",
      "\u001b[1;32m/home/yera/projects/pyth/translate/datasaur/translate.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yera/projects/pyth/translate/datasaur/translate.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m ind, _ \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yera/projects/pyth/translate/datasaur/translate.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     chunks \u001b[39m=\u001b[39m split_string_into_chunks(df\u001b[39m.\u001b[39mat[ind, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yera/projects/pyth/translate/datasaur/translate.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     df\u001b[39m.\u001b[39mat[ind, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([GoogleTranslator(source\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkk\u001b[39;49m\u001b[39m'\u001b[39;49m, target\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mtranslate(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m chunks])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/deep_translator/google.py:74\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[39mraise\u001b[39;00m TooManyRequests()\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m request_failed(status_code\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code):\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m RequestError()\n\u001b[1;32m     76\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m element \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_tag, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_query)\n",
      "\u001b[0;31mRequestError\u001b[0m: Request exception can happen due to an api connection error. Please check your connection and try again"
     ]
    }
   ],
   "source": [
    "def translate(path):\n",
    "    print(rf'{path}/*.csv')\n",
    "    for file in glob.glob(rf'{path}/*.csv'):\n",
    "        print(file)\n",
    "        df = pd.read_csv(file, index_col=\"Unnamed: 0\", engine=\"python\")\n",
    "        \n",
    "        for ind, _ in df.iterrows():\n",
    "            chunks = split_string_into_chunks(df.at[ind, \"text\"])\n",
    "            df.at[ind, \"text\"] = ' '.join([GoogleTranslator(source='kk', target='en').translate(i) for i in chunks])\n",
    "        \n",
    "\n",
    "translate(\"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
